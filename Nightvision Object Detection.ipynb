{"cells":[{"cell_type":"markdown","source":["First mount Google drive for later use\n"],"metadata":{"id":"M5PPQOdQHJA7"}},{"cell_type":"markdown","source":["Import all dependencies, first making sure that opencv, matplotlib, and numpy are installed via pip"],"metadata":{"id":"kJ6h3J4FGFPM"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"7pgBgoxXAkz2","executionInfo":{"status":"ok","timestamp":1714416866989,"user_tz":420,"elapsed":543,"user":{"displayName":"Sean Custodero","userId":"14390086787527938349"}}},"outputs":[],"source":["import cv2\n","import matplotlib\n","import numpy as np\n","from google.colab.patches import cv2_imshow\n","from itertools import combinations_with_replacement\n","from collections import defaultdict\n","from numpy.linalg import inv"]},{"cell_type":"markdown","metadata":{"id":"nPsWO-yuAqL6"},"source":["Image Enhancement; run block to define all enhancement functions"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"d7Pb3RpzD-mD","executionInfo":{"status":"ok","timestamp":1714417108677,"user_tz":420,"elapsed":305,"user":{"displayName":"Sean Custodero","userId":"14390086787527938349"}}},"outputs":[],"source":["def get_illumination_channel(I, w):\n","    M, N, _ = I.shape\n","    # padding for channels\n","    padded = np.pad(I, ((int(w/2), int(w/2)), (int(w/2), int(w/2)), (0, 0)), 'edge')\n","    darkch = np.zeros((M, N))\n","    brightch = np.zeros((M, N))\n","\n","    for i, j in np.ndindex(darkch.shape):\n","        darkch[i, j] = np.min(padded[i:i + w, j:j + w, :]) # dark channel\n","        brightch[i, j] = np.max(padded[i:i + w, j:j + w, :]) # bright channel\n","\n","    return darkch, brightch\n","\n","def get_atmosphere(I, brightch, p=0.1):\n","    M, N = brightch.shape\n","    flatI = I.reshape(M*N, 3) # reshaping image array\n","    flatbright = brightch.ravel() #flattening image array\n","\n","    searchidx = (-flatbright).argsort()[:int(M*N*p)] # sorting and slicing\n","    A = np.mean(flatI.take(searchidx, axis=0), dtype=np.float64, axis=0)\n","    return A\n","\n","def get_initial_transmission(A, brightch):\n","    A_c = np.max(A)\n","    init_t = (brightch-A_c)/(1.-A_c) # finding initial transmission map\n","    return (init_t - np.min(init_t))/(np.max(init_t) - np.min(init_t)) # normalized initial transmission map\n","\n","def get_corrected_transmission(I, A, darkch, brightch, init_t, alpha, omega, w):\n","    im = np.empty(I.shape, I.dtype);\n","    for ind in range(0, 3):\n","        im[:, :, ind] = I[:, :, ind] / A[ind] #divide pixel values by atmospheric light\n","    dark_c, _ = get_illumination_channel(im, w) # dark channel transmission map\n","    dark_t = 1 - omega*dark_c # corrected dark transmission map\n","    corrected_t = init_t # initializing corrected transmission map with initial transmission map\n","    diffch = brightch - darkch # difference between transmission maps\n","\n","    for i in range(diffch.shape[0]):\n","        for j in range(diffch.shape[1]):\n","            if(diffch[i, j] < alpha):\n","                corrected_t[i, j] = dark_t[i, j] * init_t[i, j]\n","\n","    return np.abs(corrected_t)\n","\n","R, G, B = 0, 1, 2  # index for convenience\n","\n","def boxfilter(I, r):\n","    \"\"\"Fast box filter implementation.\n","\n","    Parameters\n","    ----------\n","    I:  a single channel/gray image data normalized to [0.0, 1.0]\n","    r:  window radius\n","\n","    Return\n","    -----------\n","    The filtered image data.\n","    \"\"\"\n","    M, N = I.shape\n","    dest = np.zeros((M, N))\n","    #print(I)\n","\n","    # cumulative sum over Y axis (tate-houkou no wa)\n","    sumY = np.cumsum(I, axis=0)\n","    #print('sumY:{}'.format(sumY))\n","    # difference over Y axis\n","    dest[:r + 1] = sumY[r:2*r + 1] # top r+1 lines\n","    dest[r + 1:M - r] = sumY[2*r + 1:] - sumY[:M - 2*r - 1]\n","    #print(sumY[2*r + 1:]) # from 2*r+1 to end lines\n","    #print(sumY[:M - 2*r - 1]) # same lines of above, from start\n","    #tile replicate sumY[-1] and line them up to match the shape of (r, 1)\n","    dest[-r:] = np.tile(sumY[-1], (r, 1)) - sumY[M - 2*r - 1:M - r - 1] # bottom r lines\n","\n","    # cumulative sum over X axis\n","    sumX = np.cumsum(dest, axis=1)\n","    #print('sumX:{}'.format(sumX))\n","    # difference over X axis\n","    dest[:, :r + 1] = sumX[:, r:2*r + 1] # left r+1 columns\n","    dest[:, r + 1:N - r] = sumX[:, 2*r + 1:] - sumX[:, :N - 2*r - 1]\n","    dest[:, -r:] = np.tile(sumX[:, -1][:, None], (1, r)) - sumX[:, N - 2*r - 1:N - r - 1] # right r columns\n","\n","    #print(dest)\n","\n","    return dest\n","\n","def guided_filter(I, p, r=15, eps=1e-3):\n","    \"\"\"Refine a filter under the guidance of another (RGB) image.\n","\n","    Parameters\n","    -----------\n","    I:   an M * N * 3 RGB image for guidance.\n","    p:   the M * N filter to be guided. transmission is used for this case.\n","    r:   the radius of the guidance\n","    eps: epsilon for the guided filter\n","\n","    Return\n","    -----------\n","    The guided filter.\n","    \"\"\"\n","    M, N = p.shape\n","    base = boxfilter(np.ones((M, N)), r) # this is needed for regularization\n","\n","    # each channel of I filtered with the mean filter. this is myu.\n","    means = [boxfilter(I[:, :, i], r) / base for i in range(3)]\n","\n","    # p filtered with the mean filter\n","    mean_p = boxfilter(p, r) / base\n","\n","    # filter I with p then filter it with the mean filter\n","    means_IP = [boxfilter(I[:, :, i]*p, r) / base for i in range(3)]\n","\n","    # covariance of (I, p) in each local patch\n","    covIP = [means_IP[i] - means[i]*mean_p for i in range(3)]\n","\n","    # variance of I in each local patch: the matrix Sigma in ECCV10 eq.14\n","    var = defaultdict(dict)\n","    for i, j in combinations_with_replacement(range(3), 2):\n","        var[i][j] = boxfilter(I[:, :, i]*I[:, :, j], r) / base - means[i]*means[j]\n","\n","    a = np.zeros((M, N, 3))\n","    for y, x in np.ndindex(M, N):\n","        #         rr, rg, rb\n","        # Sigma = rg, gg, gb\n","        #         rb, gb, bb\n","        Sigma = np.array([[var[R][R][y, x], var[R][G][y, x], var[R][B][y, x]],\n","                          [var[R][G][y, x], var[G][G][y, x], var[G][B][y, x]],\n","                          [var[R][B][y, x], var[G][B][y, x], var[B][B][y, x]]])\n","        cov = np.array([c[y, x] for c in covIP])\n","        a[y, x] = np.dot(cov, inv(Sigma + eps*np.eye(3)))  # eq 14\n","\n","    # ECCV10 eq.15\n","    b = mean_p - a[:, :, R]*means[R] - a[:, :, G]*means[G] - a[:, :, B]*means[B]\n","\n","    # ECCV10 eq.16\n","    q = (boxfilter(a[:, :, R], r)*I[:, :, R] + boxfilter(a[:, :, G], r)*I[:, :, G] + boxfilter(a[:, :, B], r)*I[:, :, B] + boxfilter(b, r)) / base\n","\n","    return q\n","\n","def get_final_image(I, A, refined_t, tmin):\n","    refined_t_broadcasted = np.broadcast_to(refined_t[:, :, None], (refined_t.shape[0], refined_t.shape[1], 3)) # duplicating the channel of 2D refined map to 3 channels\n","    J = (I-A) / (np.where(refined_t_broadcasted < tmin, tmin, refined_t_broadcasted)) + A # finding result\n","\n","    return (J - np.min(J))/(np.max(J) - np.min(J)) # normalized image\n","\n","def reduce_init_t(init_t):\n","    init_t = (init_t*255).astype(np.uint8)\n","    xp = [0, 32, 255]\n","    fp = [0, 32, 48]\n","    x = np.arange(256) # creating array [0,...,255]\n","    table = np.interp(x, xp, fp).astype('uint8') # interpreting fp according to xp in range of x\n","    init_t = cv2.LUT(init_t, table) # lookup table\n","    init_t = init_t.astype(np.float64)/255 # normalizing the transmission map\n","    return init_t\n","\n","def dehaze(I, tmin=0.1, w=15, alpha=0.4, omega=0.75, p=0.1, eps=1e-3, reduce=False):\n","    I = np.asarray(I, dtype=np.float64) # Convert the input to a float array.\n","    I = I[:, :, :3] / 255\n","    m, n, _ = I.shape\n","    Idark, Ibright = get_illumination_channel(I, w)\n","    A = get_atmosphere(I, Ibright, p)\n","\n","    init_t = get_initial_transmission(A, Ibright)\n","    if reduce:\n","        init_t = reduce_init_t(init_t)\n","    corrected_t = get_corrected_transmission(I, A, Idark, Ibright, init_t, alpha, omega, w)\n","\n","    normI = (I - I.min()) / (I.max() - I.min())\n","    refined_t = guided_filter(normI, corrected_t, w, eps) # applying guided filter\n","    J_refined = get_final_image(I, A, refined_t, tmin)\n","\n","    enhanced = (J_refined*255).astype(np.uint8)\n","    f_enhanced = cv2.detailEnhance(enhanced, sigma_s=10, sigma_r=0.15)\n","    f_enhanced = cv2.edgePreservingFilter(f_enhanced, flags=1, sigma_s=64, sigma_r=0.2)\n","    return f_enhanced"]},{"cell_type":"markdown","source":[" Download the supplied 'Night_Video.mov\" file into content/drive/MyDrive/CS362V/data/"],"metadata":{"id":"uqKJxoviG45Y"}},{"cell_type":"markdown","metadata":{"id":"v0HOpU6AA2cw"},"source":["Video Capture and Object Tracking using background subtraction; run the block and the program will process the video and display the object detection on the video both before enhancement and after as a side-by-side comparison for each frame\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1W6rAFoujPVMeP-WKfyjUhONlt75-DbRV"},"executionInfo":{"elapsed":897560,"status":"ok","timestamp":1714018055965,"user":{"displayName":"Sean Custodero","userId":"14390086787527938349"},"user_tz":420},"id":"Cmy-9jKVAyqr","outputId":"917a0d4f-a54f-4374-da8e-bb719b7544a4"},"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}],"source":["# Path to video\n","cap = cv2.VideoCapture(\"/content/drive/MyDrive/CS362V/data/Night_Video.mov\")\n","\n","# Object detection from Stable camera\n","object_detector = cv2.createBackgroundSubtractorMOG2(varThreshold=20)\n","\n","# List to hold memory of previous frame for still object detection\n","prev_store = []\n","\n","# Size of the first frame\n","baseline_size = 0\n","\n","for i in range(80, 630, 12):\n","    # Select the frame\n","    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","    ret, frame = cap.read()\n","\n","    if frame is None:\n","      break\n","\n","    key = cv2.waitKey(30)\n","    if key == 27:\n","      break\n","\n","    # Create an enhanced version of the frame as a copy\n","    enhanced_frame = dehaze(frame)\n","\n","    # Check to see if first frame size has already been measured\n","    if not baseline_size:\n","      # Measure it if not\n","      baseline_size = enhanced_frame.size\n","\n","    # Object detection on both the regular frame and the enhanced version\n","    mask = object_detector.apply(frame)\n","    e_mask = object_detector.apply(enhanced_frame)\n","\n","    # Threshold both versions\n","    _, mask_thresh = cv2.threshold(mask, 180, 255, cv2.THRESH_BINARY)\n","    _, e_mask_thresh = cv2.threshold(e_mask, 180, 255, cv2.THRESH_BINARY)\n","\n","    # Kernel for erosion\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n","\n","    # Erode both versions\n","    mask_eroded = cv2.morphologyEx(mask_thresh, cv2.MORPH_OPEN, kernel)\n","    e_mask_eroded = cv2.morphologyEx(e_mask_thresh, cv2.MORPH_OPEN, kernel)\n","\n","    # Find contours in both versions\n","    contours, _ = cv2.findContours(mask_eroded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    e_contours, _ = cv2.findContours(e_mask_eroded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Keep only the contours that contain large objects for both versions\n","    min_contour_area = 500\n","    large_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_contour_area]\n","    e_large_contours = [cnt for cnt in e_contours if cv2.contourArea(cnt) > min_contour_area]\n","\n","    # Create a copy of both versions\n","    frame_out = frame.copy()\n","    e_frame_out = enhanced_frame.copy()\n","\n","    # List to track objects that were moving but stopped\n","    stationary_rects = []\n","\n","    # Checks if previous frame detected moving objects that slowed down or stopped\n","    if len(prev_store):\n","      for prev in prev_store:\n","        # Get ratio of roi from previous frame size to the baseline size\n","        roi_to_baseline = baseline_size / prev[0].size\n","\n","        # Create a grayed out roi copy of the current frame using the coordinates\n","        # of the roi passed in from the previous frame's detections\n","        gray_curr_roi = cv2.cvtColor(enhanced_frame[prev[1][1]:prev[1][3],\n","                                               prev[1][0]:prev[1][2]], cv2.COLOR_BGR2GRAY)\n","\n","        # Create a histogram of the grayed roi\n","        curr_histogram = cv2.calcHist([gray_curr_roi], [0],\n","                         None, [256], [0, 256])\n","\n","        # Create a grayed out copy of the previous frame's roi\n","        gray_prev_roi = cv2.cvtColor(prev[0], cv2.COLOR_BGR2GRAY)\n","\n","        # Histogram of the grayed out previous roi\n","        prev_histogram = cv2.calcHist([gray_prev_roi], [0],\n","                         None, [256], [0, 256])\n","\n","        # Counter to track 'difference score' between current roi and previous roi\n","        c1 = 0\n","\n","        # Loop through each pixel in each roi and multiply the differnce in\n","        # histogram values by the roi to baseline ratio to normalize values\n","        i = 0\n","        while i<len(curr_histogram) and i<len(prev_histogram):\n","            c1+=abs((curr_histogram[i]-prev_histogram[i]) * roi_to_baseline)\n","            i+= 1\n","\n","        # Border the roi in the current frame if the difference score is\n","        # less than 100000, which appears to be the range that non-motion\n","        # is calculated at\n","        if c1 < 100000:\n","          e_frame_out = cv2.rectangle(e_frame_out, (prev[1][0], prev[1][1]), (prev[1][2], prev[1][3]), (0, 255, 0), 2)\n","          stationary_rects.append(prev[1])\n","\n","    # Clear previous frame memory to prepare for remembering current frame\n","    prev_store = []\n","\n","    # Store bounding rects for each contour that was large enough\n","    rects = []\n","    rectsUsed = []\n","    for cnt in large_contours:\n","        rects.append(cv2.boundingRect(cnt))\n","        rectsUsed.append(False)\n","\n","    # Same for enhanced version\n","    e_rects = []\n","    e_rects_used = []\n","    for cnt in e_large_contours:\n","        e_rects.append(cv2.boundingRect(cnt))\n","        e_rects_used.append(False)\n","\n","    # Small helper function that returns the x value\n","    def getXFromRect(item):\n","      return item[0]\n","\n","    # Uses helper function to sort by x value\n","    rects.sort(key = getXFromRect)\n","\n","    # List of rectangles that are going to be drawn at the end\n","    acceptedRects = []\n","\n","    # Maximum horizontal distance that two bounding boxes can be in order\n","    # for them to be joined together into one large box\n","    xThr = 50\n","\n","    # Loop through each bounding rect and check if it can be joined with others\n","    for supIdx, supVal in enumerate(rects):\n","      if (rectsUsed[supIdx] == False):\n","\n","          # Initialize current rect\n","          currxMin = supVal[0]\n","          currxMax = supVal[0] + supVal[2]\n","          curryMin = supVal[1]\n","          curryMax = supVal[1] + supVal[3]\n","\n","          # This bounding rect is used\n","          rectsUsed[supIdx] = True\n","\n","          # Iterate all initial bounding rects\n","          # starting from the next\n","          for subIdx, subVal in enumerate(rects[(supIdx+1):], start = (supIdx+1)):\n","\n","              # Initialize merge candidate\n","              candxMin = subVal[0]\n","              candxMax = subVal[0] + subVal[2]\n","              candyMin = subVal[1]\n","              candyMax = subVal[1] + subVal[3]\n","\n","              # Check if x distance between current rect\n","              # and merge candidate is small enough\n","              if (candxMin <= currxMax + xThr):\n","\n","                  # Reset coordinates of current rect\n","                  currxMax = candxMax\n","                  curryMin = min(curryMin, candyMin)\n","                  curryMax = max(curryMax, candyMax)\n","\n","                  # Merge candidate (bounding rect) is used\n","                  rectsUsed[subIdx] = True\n","              else:\n","                  break\n","\n","          # No more merge candidates possible, accept current rect\n","          acceptedRects.append([currxMin, curryMin, currxMax - currxMin, curryMax - curryMin])\n","\n","    # Draw bounding boxes for regular frame\n","    for rect in acceptedRects:\n","      img = cv2.rectangle(frame_out, (rect[0], rect[1]), (rect[0] + rect[2], rect[1] + rect[3]), (121, 11, 189), 2)\n","\n","    # Repeat the actions for bounding boxes above for enhanced version\n","    e_rects.sort(key = getXFromRect)\n","\n","    e_acceptedRects = []\n","\n","    xThr = 50\n","\n","    for supIdx, supVal in enumerate(e_rects):\n","      if (e_rects_used[supIdx] == False):\n","\n","          # Initialize current rect\n","          currxMin = supVal[0]\n","          currxMax = supVal[0] + supVal[2]\n","          curryMin = supVal[1]\n","          curryMax = supVal[1] + supVal[3]\n","\n","          # This bounding rect is used\n","          e_rects_used[supIdx] = True\n","\n","          # Iterate all initial bounding rects\n","          # starting from the next\n","          for subIdx, subVal in enumerate(e_rects[(supIdx+1):], start = (supIdx+1)):\n","\n","              # Initialize merge candidate\n","              candxMin = subVal[0]\n","              candxMax = subVal[0] + subVal[2]\n","              candyMin = subVal[1]\n","              candyMax = subVal[1] + subVal[3]\n","\n","              # Check if x distance between current rect\n","              # and merge candidate is small enough\n","              if (candxMin <= currxMax + xThr):\n","\n","                  # Reset coordinates of current rect\n","                  currxMax = candxMax\n","                  curryMin = min(curryMin, candyMin)\n","                  curryMax = max(curryMax, candyMax)\n","\n","                  # Merge candidate (bounding rect) is used\n","                  e_rects_used[subIdx] = True\n","              else:\n","                  break\n","\n","          # No more merge candidates possible, accept current rect\n","          e_acceptedRects.append([currxMin, curryMin, currxMax - currxMin, curryMax - curryMin])\n","\n","    # Check to see if any of the accepted bounding boxes overlap with one of the\n","    # stationary boxes already drawn. If so, skip drawing that bounding box\n","    for rect in e_acceptedRects:\n","      x1 = rect[0]\n","      x2 = rect[2] + rect[0]\n","      y1 = rect[1]\n","      y2 = rect[3] + rect[1]\n","\n","      valid = True\n","      # Loop through stationary rects\n","      if len(stationary_rects):\n","        for s_rect in stationary_rects:\n","          # Check for overlap\n","          if s_rect[0] <= rect[0] <= s_rect[2] or s_rect[0] <= rect[2] <= s_rect[2] or s_rect[1] <= rect[1] <= s_rect[3] or s_rect[1] <= rect[3] <= s_rect[3]:\n","            valid = False\n","            break\n","        if valid:\n","          # Draw box if no overlaps\n","          e_frame_out = cv2.rectangle(e_frame_out, (x1, y1), (x2, y2), (121, 11, 189), 2)\n","      else:\n","          # Draw box if there were no stationary boxes\n","        e_frame_out = cv2.rectangle(e_frame_out, (x1, y1), (x2, y2), (121, 11, 189), 2)\n","\n","     # Add all the detections to the previous frame memory\n","      prev_store.append((enhanced_frame[y1:y2, x1:x2], [x1, y1, x2, y2]))\n","\n","    # Join the regualar and enhanced frames to see side by side comparison\n","    joint = np.hstack([frame_out, e_frame_out])\n","    cv2_imshow(joint)\n","\n","\n","\n","cap.release()"]},{"cell_type":"markdown","source":["Same thing but using the video with stops to check stationary object detection. A green box around an object means it was previously moving but now stopped"],"metadata":{"id":"NXNZrNS7N3cQ"}},{"cell_type":"code","source":["# Path to video\n","cap = cv2.VideoCapture(\"/content/drive/MyDrive/CS362V/data/Night_Video_With_Stops.mov\")\n","\n","# Object detection from Stable camera\n","object_detector = cv2.createBackgroundSubtractorMOG2(varThreshold=20)\n","\n","# List to hold memory of previous frame for still object detection\n","prev_store = []\n","\n","# Size of the first frame\n","baseline_size = 0\n","\n","for i in range(80, 1000, 12):\n","    # Select the frame\n","    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","    ret, frame = cap.read()\n","\n","    if frame is None:\n","      break\n","\n","    key = cv2.waitKey(30)\n","    if key == 27:\n","      break\n","\n","    # Create an enhanced version of the frame as a copy\n","    enhanced_frame = dehaze(frame)\n","\n","    # Check to see if first frame size has already been measured\n","    if not baseline_size:\n","      # Measure it if not\n","      baseline_size = enhanced_frame.size\n","\n","    # Object detection on both the regular frame and the enhanced version\n","    mask = object_detector.apply(frame)\n","    e_mask = object_detector.apply(enhanced_frame)\n","\n","    # Threshold both versions\n","    _, mask_thresh = cv2.threshold(mask, 180, 255, cv2.THRESH_BINARY)\n","    _, e_mask_thresh = cv2.threshold(e_mask, 180, 255, cv2.THRESH_BINARY)\n","\n","    # Kernel for erosion\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n","\n","    # Erode both versions\n","    mask_eroded = cv2.morphologyEx(mask_thresh, cv2.MORPH_OPEN, kernel)\n","    e_mask_eroded = cv2.morphologyEx(e_mask_thresh, cv2.MORPH_OPEN, kernel)\n","\n","    # Find contours in both versions\n","    contours, _ = cv2.findContours(mask_eroded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    e_contours, _ = cv2.findContours(e_mask_eroded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Keep only the contours that contain large objects for both versions\n","    min_contour_area = 500\n","    large_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_contour_area]\n","    e_large_contours = [cnt for cnt in e_contours if cv2.contourArea(cnt) > min_contour_area]\n","\n","    # Create a copy of both versions\n","    frame_out = frame.copy()\n","    e_frame_out = enhanced_frame.copy()\n","\n","    # List to track objects that were moving but stopped\n","    stationary_rects = []\n","\n","    # Checks if previous frame detected moving objects that slowed down or stopped\n","    if len(prev_store):\n","      for prev in prev_store:\n","        # Get ratio of roi from previous frame size to the baseline size\n","        roi_to_baseline = baseline_size / prev[0].size\n","\n","        # Create a grayed out roi copy of the current frame using the coordinates\n","        # of the roi passed in from the previous frame's detections\n","        gray_curr_roi = cv2.cvtColor(enhanced_frame[prev[1][1]:prev[1][3],\n","                                               prev[1][0]:prev[1][2]], cv2.COLOR_BGR2GRAY)\n","\n","        # Create a histogram of the grayed roi\n","        curr_histogram = cv2.calcHist([gray_curr_roi], [0],\n","                         None, [256], [0, 256])\n","\n","        # Create a grayed out copy of the previous frame's roi\n","        gray_prev_roi = cv2.cvtColor(prev[0], cv2.COLOR_BGR2GRAY)\n","\n","        # Histogram of the grayed out previous roi\n","        prev_histogram = cv2.calcHist([gray_prev_roi], [0],\n","                         None, [256], [0, 256])\n","\n","        # Counter to track 'difference score' between current roi and previous roi\n","        c1 = 0\n","\n","        # Loop through each pixel in each roi and multiply the differnce in\n","        # histogram values by the roi to baseline ratio to normalize values\n","        i = 0\n","        while i<len(curr_histogram) and i<len(prev_histogram):\n","            c1+=abs((curr_histogram[i]-prev_histogram[i]) * roi_to_baseline)\n","            i+= 1\n","\n","        # Border the roi in the current frame if the difference score is\n","        # less than 100000, which appears to be the range that non-motion\n","        # is calculated at\n","        if c1 < 100000:\n","          e_frame_out = cv2.rectangle(e_frame_out, (prev[1][0], prev[1][1]), (prev[1][2], prev[1][3]), (0, 255, 0), 2)\n","          stationary_rects.append(prev[1])\n","\n","    # Clear previous frame memory to prepare for remembering current frame\n","    prev_store = []\n","\n","    # Store bounding rects for each contour that was large enough\n","    rects = []\n","    rectsUsed = []\n","    for cnt in large_contours:\n","        rects.append(cv2.boundingRect(cnt))\n","        rectsUsed.append(False)\n","\n","    # Same for enhanced version\n","    e_rects = []\n","    e_rects_used = []\n","    for cnt in e_large_contours:\n","        e_rects.append(cv2.boundingRect(cnt))\n","        e_rects_used.append(False)\n","\n","    # Small helper function that returns the x value\n","    def getXFromRect(item):\n","      return item[0]\n","\n","    # Uses helper function to sort by x value\n","    rects.sort(key = getXFromRect)\n","\n","    # List of rectangles that are going to be drawn at the end\n","    acceptedRects = []\n","\n","    # Maximum horizontal distance that two bounding boxes can be in order\n","    # for them to be joined together into one large box\n","    xThr = 50\n","\n","    # Loop through each bounding rect and check if it can be joined with others\n","    for supIdx, supVal in enumerate(rects):\n","      if (rectsUsed[supIdx] == False):\n","\n","          # Initialize current rect\n","          currxMin = supVal[0]\n","          currxMax = supVal[0] + supVal[2]\n","          curryMin = supVal[1]\n","          curryMax = supVal[1] + supVal[3]\n","\n","          # This bounding rect is used\n","          rectsUsed[supIdx] = True\n","\n","          # Iterate all initial bounding rects\n","          # starting from the next\n","          for subIdx, subVal in enumerate(rects[(supIdx+1):], start = (supIdx+1)):\n","\n","              # Initialize merge candidate\n","              candxMin = subVal[0]\n","              candxMax = subVal[0] + subVal[2]\n","              candyMin = subVal[1]\n","              candyMax = subVal[1] + subVal[3]\n","\n","              # Check if x distance between current rect\n","              # and merge candidate is small enough\n","              if (candxMin <= currxMax + xThr):\n","\n","                  # Reset coordinates of current rect\n","                  currxMax = candxMax\n","                  curryMin = min(curryMin, candyMin)\n","                  curryMax = max(curryMax, candyMax)\n","\n","                  # Merge candidate (bounding rect) is used\n","                  rectsUsed[subIdx] = True\n","              else:\n","                  break\n","\n","          # No more merge candidates possible, accept current rect\n","          acceptedRects.append([currxMin, curryMin, currxMax - currxMin, curryMax - curryMin])\n","\n","    # Draw bounding boxes for regular frame\n","    for rect in acceptedRects:\n","      img = cv2.rectangle(frame_out, (rect[0], rect[1]), (rect[0] + rect[2], rect[1] + rect[3]), (121, 11, 189), 2)\n","\n","    # Repeat the actions for bounding boxes above for enhanced version\n","    e_rects.sort(key = getXFromRect)\n","\n","    e_acceptedRects = []\n","\n","    xThr = 50\n","\n","    for supIdx, supVal in enumerate(e_rects):\n","      if (e_rects_used[supIdx] == False):\n","\n","          # Initialize current rect\n","          currxMin = supVal[0]\n","          currxMax = supVal[0] + supVal[2]\n","          curryMin = supVal[1]\n","          curryMax = supVal[1] + supVal[3]\n","\n","          # This bounding rect is used\n","          e_rects_used[supIdx] = True\n","\n","          # Iterate all initial bounding rects\n","          # starting from the next\n","          for subIdx, subVal in enumerate(e_rects[(supIdx+1):], start = (supIdx+1)):\n","\n","              # Initialize merge candidate\n","              candxMin = subVal[0]\n","              candxMax = subVal[0] + subVal[2]\n","              candyMin = subVal[1]\n","              candyMax = subVal[1] + subVal[3]\n","\n","              # Check if x distance between current rect\n","              # and merge candidate is small enough\n","              if (candxMin <= currxMax + xThr):\n","\n","                  # Reset coordinates of current rect\n","                  currxMax = candxMax\n","                  curryMin = min(curryMin, candyMin)\n","                  curryMax = max(curryMax, candyMax)\n","\n","                  # Merge candidate (bounding rect) is used\n","                  e_rects_used[subIdx] = True\n","              else:\n","                  break\n","\n","          # No more merge candidates possible, accept current rect\n","          e_acceptedRects.append([currxMin, curryMin, currxMax - currxMin, curryMax - curryMin])\n","\n","    # Check to see if any of the accepted bounding boxes overlap with one of the\n","    # stationary boxes already drawn. If so, skip drawing that bounding box\n","    for rect in e_acceptedRects:\n","      x1 = rect[0]\n","      x2 = rect[2] + rect[0]\n","      y1 = rect[1]\n","      y2 = rect[3] + rect[1]\n","\n","      valid = True\n","      # Loop through stationary rects\n","      if len(stationary_rects):\n","        for s_rect in stationary_rects:\n","          # Check for overlap\n","          if s_rect[0] <= rect[0] <= s_rect[2] or s_rect[0] <= rect[2] <= s_rect[2] or s_rect[1] <= rect[1] <= s_rect[3] or s_rect[1] <= rect[3] <= s_rect[3]:\n","            valid = False\n","            break\n","        if valid:\n","          # Draw box if no overlaps\n","          e_frame_out = cv2.rectangle(e_frame_out, (x1, y1), (x2, y2), (121, 11, 189), 2)\n","      else:\n","          # Draw box if there were no stationary boxes\n","        e_frame_out = cv2.rectangle(e_frame_out, (x1, y1), (x2, y2), (121, 11, 189), 2)\n","\n","     # Add all the detections to the previous frame memory\n","      prev_store.append((enhanced_frame[y1:y2, x1:x2], [x1, y1, x2, y2]))\n","\n","    # Join the regualar and enhanced frames to see side by side comparison\n","    joint = np.hstack([frame_out, e_frame_out])\n","    cv2_imshow(joint)\n","\n","\n","\n","cap.release()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1K-AVs7B3Qwm4PcqKZ1lhDIeBDU1YHFWl"},"id":"nnGqcMxCN3D0","executionInfo":{"status":"ok","timestamp":1714418319674,"user_tz":420,"elapsed":1151918,"user":{"displayName":"Sean Custodero","userId":"14390086787527938349"}},"outputId":"593e9209-2a36-4012-e2bc-47ff645146a0"},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","source":["Object detection on daytime video without enhancement. Regular object detection on left and object detection plus stationary detection on right."],"metadata":{"id":"gRFCduLJVp7y"}},{"cell_type":"code","source":["# Path to video\n","cap = cv2.VideoCapture(\"/content/drive/MyDrive/CS362V/data/Day_Video_With_Stops.mov\")\n","\n","# Object detection from Stable camera\n","object_detector = cv2.createBackgroundSubtractorMOG2(varThreshold=20)\n","\n","# List to hold memory of previous frame for still object detection\n","prev_store = []\n","\n","# Size of the first frame\n","baseline_size = 0\n","\n","for i in range(80, 1000, 12):\n","    # Select the frame\n","    cap.set(cv2.CAP_PROP_POS_FRAMES, i)\n","    ret, frame = cap.read()\n","\n","    if frame is None:\n","      break\n","\n","    key = cv2.waitKey(30)\n","    if key == 27:\n","      break\n","\n","    # Create an enhanced version of the frame as a copy\n","    enhanced_frame = frame.copy()\n","\n","    # Check to see if first frame size has already been measured\n","    if not baseline_size:\n","      # Measure it if not\n","      baseline_size = enhanced_frame.size\n","\n","    # Object detection on both the regular frame and the enhanced version\n","    mask = object_detector.apply(frame)\n","    e_mask = object_detector.apply(enhanced_frame)\n","\n","    # Threshold both versions\n","    _, mask_thresh = cv2.threshold(mask, 180, 255, cv2.THRESH_BINARY)\n","    _, e_mask_thresh = cv2.threshold(e_mask, 180, 255, cv2.THRESH_BINARY)\n","\n","    # Kernel for erosion\n","    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n","\n","    # Erode both versions\n","    mask_eroded = cv2.morphologyEx(mask_thresh, cv2.MORPH_OPEN, kernel)\n","    e_mask_eroded = cv2.morphologyEx(e_mask_thresh, cv2.MORPH_OPEN, kernel)\n","\n","    # Find contours in both versions\n","    contours, _ = cv2.findContours(mask_eroded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","    e_contours, _ = cv2.findContours(e_mask_eroded, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n","\n","    # Keep only the contours that contain large objects for both versions\n","    min_contour_area = 500\n","    large_contours = [cnt for cnt in contours if cv2.contourArea(cnt) > min_contour_area]\n","    e_large_contours = [cnt for cnt in e_contours if cv2.contourArea(cnt) > min_contour_area]\n","\n","    # Create a copy of both versions\n","    frame_out = frame.copy()\n","    e_frame_out = enhanced_frame.copy()\n","\n","    # List to track objects that were moving but stopped\n","    stationary_rects = []\n","\n","    # Checks if previous frame detected moving objects that slowed down or stopped\n","    if len(prev_store):\n","      for prev in prev_store:\n","        # Get ratio of roi from previous frame size to the baseline size\n","        roi_to_baseline = baseline_size / prev[0].size\n","\n","        # Create a grayed out roi copy of the current frame using the coordinates\n","        # of the roi passed in from the previous frame's detections\n","        gray_curr_roi = cv2.cvtColor(enhanced_frame[prev[1][1]:prev[1][3],\n","                                               prev[1][0]:prev[1][2]], cv2.COLOR_BGR2GRAY)\n","\n","        # Create a histogram of the grayed roi\n","        curr_histogram = cv2.calcHist([gray_curr_roi], [0],\n","                         None, [256], [0, 256])\n","\n","        # Create a grayed out copy of the previous frame's roi\n","        gray_prev_roi = cv2.cvtColor(prev[0], cv2.COLOR_BGR2GRAY)\n","\n","        # Histogram of the grayed out previous roi\n","        prev_histogram = cv2.calcHist([gray_prev_roi], [0],\n","                         None, [256], [0, 256])\n","\n","        # Counter to track 'difference score' between current roi and previous roi\n","        c1 = 0\n","\n","        # Loop through each pixel in each roi and multiply the differnce in\n","        # histogram values by the roi to baseline ratio to normalize values\n","        i = 0\n","        while i<len(curr_histogram) and i<len(prev_histogram):\n","            c1+=abs((curr_histogram[i]-prev_histogram[i]) * roi_to_baseline)\n","            i+= 1\n","\n","        # Border the roi in the current frame if the difference score is\n","        # less than 100000, which appears to be the range that non-motion\n","        # is calculated at\n","        if c1 < 100000:\n","          e_frame_out = cv2.rectangle(e_frame_out, (prev[1][0], prev[1][1]), (prev[1][2], prev[1][3]), (0, 255, 0), 2)\n","          stationary_rects.append(prev[1])\n","\n","    # Clear previous frame memory to prepare for remembering current frame\n","    prev_store = []\n","\n","    # Store bounding rects for each contour that was large enough\n","    rects = []\n","    rectsUsed = []\n","    for cnt in large_contours:\n","        rects.append(cv2.boundingRect(cnt))\n","        rectsUsed.append(False)\n","\n","    # Same for enhanced version\n","    e_rects = []\n","    e_rects_used = []\n","    for cnt in e_large_contours:\n","        e_rects.append(cv2.boundingRect(cnt))\n","        e_rects_used.append(False)\n","\n","    # Small helper function that returns the x value\n","    def getXFromRect(item):\n","      return item[0]\n","\n","    # Uses helper function to sort by x value\n","    rects.sort(key = getXFromRect)\n","\n","    # List of rectangles that are going to be drawn at the end\n","    acceptedRects = []\n","\n","    # Maximum horizontal distance that two bounding boxes can be in order\n","    # for them to be joined together into one large box\n","    xThr = 50\n","\n","    # Loop through each bounding rect and check if it can be joined with others\n","    for supIdx, supVal in enumerate(rects):\n","      if (rectsUsed[supIdx] == False):\n","\n","          # Initialize current rect\n","          currxMin = supVal[0]\n","          currxMax = supVal[0] + supVal[2]\n","          curryMin = supVal[1]\n","          curryMax = supVal[1] + supVal[3]\n","\n","          # This bounding rect is used\n","          rectsUsed[supIdx] = True\n","\n","          # Iterate all initial bounding rects\n","          # starting from the next\n","          for subIdx, subVal in enumerate(rects[(supIdx+1):], start = (supIdx+1)):\n","\n","              # Initialize merge candidate\n","              candxMin = subVal[0]\n","              candxMax = subVal[0] + subVal[2]\n","              candyMin = subVal[1]\n","              candyMax = subVal[1] + subVal[3]\n","\n","              # Check if x distance between current rect\n","              # and merge candidate is small enough\n","              if (candxMin <= currxMax + xThr):\n","\n","                  # Reset coordinates of current rect\n","                  currxMax = candxMax\n","                  curryMin = min(curryMin, candyMin)\n","                  curryMax = max(curryMax, candyMax)\n","\n","                  # Merge candidate (bounding rect) is used\n","                  rectsUsed[subIdx] = True\n","              else:\n","                  break\n","\n","          # No more merge candidates possible, accept current rect\n","          acceptedRects.append([currxMin, curryMin, currxMax - currxMin, curryMax - curryMin])\n","\n","    # Draw bounding boxes for regular frame\n","    for rect in acceptedRects:\n","      img = cv2.rectangle(frame_out, (rect[0], rect[1]), (rect[0] + rect[2], rect[1] + rect[3]), (121, 11, 189), 2)\n","\n","    # Repeat the actions for bounding boxes above for enhanced version\n","    e_rects.sort(key = getXFromRect)\n","\n","    e_acceptedRects = []\n","\n","    xThr = 50\n","\n","    for supIdx, supVal in enumerate(e_rects):\n","      if (e_rects_used[supIdx] == False):\n","\n","          # Initialize current rect\n","          currxMin = supVal[0]\n","          currxMax = supVal[0] + supVal[2]\n","          curryMin = supVal[1]\n","          curryMax = supVal[1] + supVal[3]\n","\n","          # This bounding rect is used\n","          e_rects_used[supIdx] = True\n","\n","          # Iterate all initial bounding rects\n","          # starting from the next\n","          for subIdx, subVal in enumerate(e_rects[(supIdx+1):], start = (supIdx+1)):\n","\n","              # Initialize merge candidate\n","              candxMin = subVal[0]\n","              candxMax = subVal[0] + subVal[2]\n","              candyMin = subVal[1]\n","              candyMax = subVal[1] + subVal[3]\n","\n","              # Check if x distance between current rect\n","              # and merge candidate is small enough\n","              if (candxMin <= currxMax + xThr):\n","\n","                  # Reset coordinates of current rect\n","                  currxMax = candxMax\n","                  curryMin = min(curryMin, candyMin)\n","                  curryMax = max(curryMax, candyMax)\n","\n","                  # Merge candidate (bounding rect) is used\n","                  e_rects_used[subIdx] = True\n","              else:\n","                  break\n","\n","          # No more merge candidates possible, accept current rect\n","          e_acceptedRects.append([currxMin, curryMin, currxMax - currxMin, curryMax - curryMin])\n","\n","    # Check to see if any of the accepted bounding boxes overlap with one of the\n","    # stationary boxes already drawn. If so, skip drawing that bounding box\n","    for rect in e_acceptedRects:\n","      x1 = rect[0]\n","      x2 = rect[2] + rect[0]\n","      y1 = rect[1]\n","      y2 = rect[3] + rect[1]\n","\n","      valid = True\n","      # Loop through stationary rects\n","      if len(stationary_rects):\n","        for s_rect in stationary_rects:\n","          # Check for overlap\n","          if s_rect[0] <= rect[0] <= s_rect[2] or s_rect[0] <= rect[2] <= s_rect[2] or s_rect[1] <= rect[1] <= s_rect[3] or s_rect[1] <= rect[3] <= s_rect[3]:\n","            valid = False\n","            break\n","        if valid:\n","          # Draw box if no overlaps\n","          e_frame_out = cv2.rectangle(e_frame_out, (x1, y1), (x2, y2), (121, 11, 189), 2)\n","      else:\n","          # Draw box if there were no stationary boxes\n","        e_frame_out = cv2.rectangle(e_frame_out, (x1, y1), (x2, y2), (121, 11, 189), 2)\n","\n","     # Add all the detections to the previous frame memory\n","      prev_store.append((enhanced_frame[y1:y2, x1:x2], [x1, y1, x2, y2]))\n","\n","    # Join the regualar and enhanced frames to see side by side comparison\n","    joint = np.hstack([frame_out, e_frame_out])\n","    cv2_imshow(joint)\n","\n","\n","\n","cap.release()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1sUYlug6ZGSt_dsdV0r98oGXcfWfxR9VV"},"id":"ft5ZXTwAVptS","executionInfo":{"status":"ok","timestamp":1714419271547,"user_tz":420,"elapsed":64454,"user":{"displayName":"Sean Custodero","userId":"14390086787527938349"}},"outputId":"07bdfe2b-779c-41d5-95a0-72f8eb081e62"},"execution_count":8,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]}],"metadata":{"colab":{"gpuType":"T4","provenance":[],"mount_file_id":"1JphsRaQjmmNWD0SUJebhu-B9q2_cuGff","authorship_tag":"ABX9TyOoiZSn9zvnS/XZjByZ7/3Y"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}